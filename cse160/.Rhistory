test$V17 <- NULL
test$V18 <- NULL
glmModel <- glm(V1 ~ ., family = binomial(link='logit'), data = train)
#predict test data
glmPred <- predict(glmModel, test, type='response')
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(survived ~ ., train = train, test = test, k = round(nrow(train)^.5))
install.packages("kknn")
library(kknn)
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(survived ~ ., train = train, test = test, k = round(nrow(train)^.5))
install.packages("kknn")
library(kknn)
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
#predict test data
kknnPred <- predict(kknnModel)
#clasify the predictions
kknnPred <- ifelse(kknnPred > 0.5,1,0)
#create confusion matrix
table(kknnPred != 0, test$survived != 0, dnn = c("pred.", "act."))
install.packages("kknn")
install.packages("kknn")
library(kknn)
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
install.packages("kknn")
#predict test data
kknnPred <- predict(kknnModel)
#clasify the predictions
kknnPred <- ifelse(kknnPred > 0.5,1,0)
#create confusion matrix
conf <- table(kknnPred, test$V1, dnn = c("pred.", "act."))
#calculate accuracy
print(paste('Accuracy:', mean(kknnPred == test$survived)))
#install.packages("kknn")
#library(kknn)
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
#predict test data
kknnPred <- predict(kknnModel)
#clasify the predictions
kknnPred <- ifelse(kknnPred > 0.5,1,0)
#create confusion matrix
conf <- table(kknnPred, test$V1, dnn = c("pred.", "act."))
conf
#calculate accuracy
print(paste('Accuracy:', mean(kknnPred == test$survived)))
#install.packages("kknn")
#library(kknn)
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
#predict test data
kknnPred <- predict(kknnModel)
#create confusion matrix
conf <- table(kknnPred, test$V1, dnn = c("pred.", "act."))
conf
#calculate accuracy
print(paste('Accuracy:', mean(kknnPred == test$survived)))
#install.packages("kknn")
#library(kknn)
#remove constants
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
#predict test data
kknnPred <- predict(kknnModel)
#create confusion matrix
conf <- table(kknnPred, test$V1, dnn = c("pred.", "act."))
#calculate accuracy
print(paste('Accuracy:', mean(kknnPred == test$V1)))
#install.packages("kknn")
#library(kknn)
#remove constants
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
#predict test data
kknnPred <- predict(kknnModel)
#create confusion matrix
conf <- table(kknnPred, test$V1, dnn = c("pred.", "act."))
#calculate accuracy
print(paste('Accuracy:', mean(kknnPred == test$V1)))
tp <- conf[1,1]
fp <- conf[1,2]
fn <- conf[2,1]
tn <- conf[2,2]
prec <- tp / (tp + fp)
recall <- tp / (tp + fn)
print(paste('Precision:', prec))
print(paste('Recall:', recall))
#install.packages("kknn")
library(kknn)
#remove constants
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
#predict test data
kknnPred <- predict(kknnModel)
#create confusion matrix
conf <- table(kknnPred, test$V1, dnn = c("pred.", "act."))
#calculate accuracy
print(paste('Accuracy:', mean(kknnPred == test$V1)))
tp <- conf[1,1]
fp <- conf[1,2]
fn <- conf[2,1]
tn <- conf[2,2]
prec <- tp / (tp + fp)
recall <- tp / (tp + fn)
print(paste('Precision:', prec))
print(paste('Recall:', recall))
version
# YOUR CODE GOES HERE
small <- read.csv(http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv)
# YOUR CODE GOES HERE
small <- read.csv("http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv")
# YOUR CODE GOES HERE
small <- read.csv("http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE)
# YOUR CODE GOES HERE
small <- read.csv("http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = FALSE)
# YOUR CODE GOES HERE
small <- read.csv("http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv")
# YOUR CODE GOES HERE
small <- read.csv("http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv", header = TRUE, sep=",")
# YOUR CODE GOES HERE
small <- read.table("http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv", header = TRUE, sep=",")
# YOUR CODE GOES HERE
small <- read.table(file = "http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv", header = TRUE, sep=",")
# YOUR CODE GOES HERE
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
# YOUR CODE GOES HERE
small = read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
# YOUR CODE GOES HERE
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
help()
t(small)
small <- t(small)
# YOUR CODE GOES HERE
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
# YOUR CODE GOES HERE
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
# YOUR CODE GOES HERE
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
help(wordcloud)
# YOUR CODE GOES HERE
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
library(wordcloud)
install.packages("wordcloud")
install.packages("tm")
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
library(wordcloud)
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
help("wordcloud")
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
sum(small[,2:nrow(small)])
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
colSums(small)
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
rowSums(small)
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
freq <- rowSums(small)
freq[1]
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
freq <- rowSums(small)
wordcloud(rownames(small), freq, min.freq = 5, max.words = 80)
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
freq <- rowSums(small)
wordcloud(rownames(small), freq, min.freq = 5, max.words = 80)
large <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(large) <- large$X
large$X <- NULL
freql <- rowSums(large)
wordcloud(rownames(large), freql, min.freq = 5, max.words = 80)
help(tm)
# Your code to create a document term matrix goes here
library(tm)
help(tm)
# Your code to create a document term matrix goes here
library(topicmodels)
# Your code to create a document term matrix goes here
package.install(topicmodels)
# Your code to create a document term matrix goes here
install.packages(topicmodels)
library(topicmodels)
# Your code to create a document term matrix goes here
library(tm)
median(freq)
# Your code to create a document term matrix goes here
library(tm)
subset(freq, freq > median(freq))
# Your code to create a document term matrix goes here
library(tm)
rownames(subset(freq, freq > median(freq)))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
freq
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
freq <- rowSums(small)
wordcloud(rownames(small), freq, min.freq = 5, max.words = 80)
large <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(large) <- large$X
large$X <- NULL
freql <- rowSums(large)
wordcloud(rownames(large), freql, min.freq = 5, max.words = 80)
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
freq
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
data.frame(freq)
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
rownames(data.frame(freq))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq))
small <- subset(small, rownames(small) %in% rownames(data.frame(freq)))
small
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
freq <- rowSums(small)
wordcloud(rownames(small), freq, min.freq = 5, max.words = 80)
large <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(large) <- large$X
large$X <- NULL
freql <- rowSums(large)
wordcloud(rownames(large), freql, min.freq = 5, max.words = 80)
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq)) #get the upper half of terms
small <- subset(small, rownames(small) %in% rownames(data.frame(freq))) #drop lower half
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63)
nstart <- 3
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
# Your code to create a document term matrix goes here
library(tm)
library(LDA)
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
freq <- rowSums(small)
wordcloud(rownames(small), freq, min.freq = 5, max.words = 80)
large <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(large) <- large$X
large$X <- NULL
freql <- rowSums(large)
wordcloud(rownames(large), freql, min.freq = 5, max.words = 80)
# Your code to create a document term matrix goes here
library(tm)
freq <- subset(freq, freq > median(freq)) #get the upper half of terms
small <- subset(small, rownames(small) %in% rownames(data.frame(freq))) #drop lower half
dtm <- as.DocumentTermMatrix(t(small), weightTf)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63)
nstart <- 3
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
install.packages('topicmodels')
# YOUR CODE GOES HERE
library(wordcloud)
small <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(small) <- small$X
small$X <- NULL
freq <- rowSums(small)
wordcloud(rownames(small), freq, min.freq = 5, max.words = 80)
large <- read.table(file="http://www.cse.lehigh.edu/~brian/course/2020/datascience/Conf_small.csv",header = TRUE, sep = ",")
rownames(large) <- large$X
large$X <- NULL
freql <- rowSums(large)
wordcloud(rownames(large), freql, min.freq = 5, max.words = 80)
# Your code to create a document term matrix goes here
library(tm)
library(topicmodels)
freq <- subset(freq, freq > median(freq)) #get the upper half of terms
small <- subset(small, rownames(small) %in% rownames(data.frame(freq))) #drop lower half
dtm <- as.DocumentTermMatrix(t(small), weightTf)
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63)
nstart <- 3
best <- TRUE
#Number of topics
k <- 10
#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
#Capture the top 20 terms for each topic
ldaOut.terms <- as.matrix(terms(ldaOut,20))
#install.packages("kknn")
library(kknn)
#remove constants
train$V7 <- NULL
mushroom <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data", sep = ",");
n <- nrow(mushroom)
mid <- round(n / 2)
train <- mushroom[1:mid,]
test <- mushroom[(mid+1):n,]
library(rpart)
mushroomTree <- rpart(V1 ~ ., data = train, method = 'class')
mPred <- predict(mushroomTree, test, type='class')
acc <- mean(mPred == test$V1)
print(paste('Accuracy:', acc))
conf <- table(mPred, test$V1, dnn = c("pred.", "act."))
tp <- conf[1,1]
fp <- conf[1,2]
fn <- conf[2,1]
tn <- conf[2,2]
prec <- tp / (tp + fp)
recall <- tp / (tp + fn)
print(paste('Precision:', prec))
print(paste('Recall:', recall))
#install.packages("kknn")
library(kknn)
#remove constants
train$V7 <- NULL
train$V17 <- NULL
train$V18 <- NULL
test$V7 <- NULL
test$V17 <- NULL
test$V18 <- NULL
#K-Nearest Neighbor
#generate model, setting k = sqrt(n) where n is rows in training data
kknnModel <- kknn(V1 ~ ., train = train, test = test, k = round(nrow(train)^.5))
#predict test data
kknnPred <- predict(kknnModel)
#create confusion matrix
conf <- table(kknnPred, test$V1, dnn = c("pred.", "act."))
conf
#calculate accuracy
print(paste('Accuracy:', mean(kknnPred == test$V1)))
tp <- conf[1,1]
fp <- conf[1,2]
fn <- conf[2,1]
tn <- conf[2,2]
prec <- tp / (tp + fp)
recall <- tp / (tp + fn)
print(paste('Precision:', prec))
print(paste('Recall:', recall))
